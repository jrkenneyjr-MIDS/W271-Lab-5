---
title: "Lab 5"
author: "Ram Balasubramanian, John Kenney"
date: "December 14, 2017"
output: pdf_document
---

```{r include = FALSE}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=70),tidy=TRUE, warning=FALSE, options(digits = 4))

library(Hmisc)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(maps)
library(mapproj)
library(viridis)
```

**Read and Examine the data:  Load the data.** 

```{r}
load("driving.RData")
traffic = data
#View(desc)
#View(traffic)
#table(with(data,state,year))

#hist(traffic$minage)
#length(unique(traffic$minage))

#hist(traffic$zerotol)
#length(unique(traffic$zerotol))

```
#Question 1
****************
Provide a description of the basic structure of the dataset, as we have done throughout the semester. Be sure your explanation includes what the variable means and how it is coded.
****************

##1.1 Description of Dataset

This data is structured as a long panel set, where each of the 48 states continental states are numbered alphabetically from 1 to 51, with 2, 9, and 12 missing (Alaska, Hawaii, District of Columbia).  Each state has associated with it 25 annual observations ranging from 1980 to 2004.  The year is both indicated as its own variable and represented as one of 25 dummy variables. Within each Year-by-State observation there are observations that describe the state's traffic laws, traffic fatalities, and population demographics.

Variables that are coded as dummy (1 or 0) will often show a number between these two dichotomous options.  This indicates that the state's law was changed during this year and the variable was likely aggregated as the average of monthly observations.  The percentage indicates the amount of time for which the law was active.  For example, a value of 0.75 indicates that for three quarters of the year, the variable was True and for the other quarter, the variable was False.

###1.1.1 Traffic Laws

####1.1.1.1 Speed Limit
There are six dummy variables starting with "sl", which indicate the speed limit mandated by the state for the year. The first four variables code speed limits in 5 mph increments from 55 to 75 mph.  The fifth variable "slnone" indicates there was no speed limit for the state that year.  The sixth variable 'sl70plus' indicates that either the speed limit was 70 mph or greater, or that there was no speed limit that year.  


####1.1.1.2 Seatbelts
The next variable "seatbelt" is categorical and describes the type of seat belt law that exists, "0" if no law, '1' if primary (no other violation required to give a ticket), "2" if secondary (another violation must have occurred for the officer to issue a seatbelt ticket).  There also exist two dummy variables starting with "sb", one for primary and the other for secondary.


####1.1.1.3 Drinking
The variables "minage", "zerotol", and 'bac' describe the state's approach to drinking laws.  The 'minage' variable is the state's legal drinking age for the year, taking on 12 distinct values from ranging from 18 to 21, with 21 making up the great majority of observations.  Non-integer observations indicate a year of 

The 'zerotol' variable indicates if the state enacted a Zero Tolerance law for drinking, which makes it a criminal DUI offense for drivers under the age of 21 to drive with even a small amount of alcohol in their system.  This variable takes on 11 distinct values, ranging from 0 to 1, with zero and 1 making up the vast majority of observations

The next two "bac" dummy variables indicate if the state's acceptable BAC is 0.08% or 0.10%.

####1.1.1.4 Perse
Several states have adopted "per se" laws which allows for suspension or revocation of driver's license for DUI or DWI cases.  

###1.1.2 Fatality Statistics

Fatality statistics are given for each State-by-Year observation and include gross totals as well as totals normalized by vehicle mile driven and per capita. 

These statistics are reported in terms of the time of occurrence, which include fatalities over all times, fatalities at nighttime, and fatalities during the weekend.  

####1.1.3 Demographics

Observations also report a number of demographics specific to each state for each year reported.  These include the number of vehicle miles in billions by the state's population for the year (total and per capita), the state's percent unemployment, and the percentage of the state's population between 14 and 24, inclusive.

##1.2 Exploratory Data Analysis
****************
Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable totfatrte and the potential explanatory variables. 

Remember, graphs must be well-labeled.

You need to write a detailed narrative of your observations of your EDA. 

Reminder: giving an "output dump" (i.e. providing a bunch of graphs and tables without description and hoping your audience will interpret them) will receive zero point in this exercise.
****************

###1.2.1 Univariate EDA

####1.2.1.1 Dependent Variables - Fatalaties
Visualizations:
Ideas: 3 x 3 Histogram Matrix of overall distribution
Heat Map by State 3 x 3 Geoplot 
3 x 3 Time Series of Averages

Statistics:
describe()
mean, mode, median
distribution type estimator
  Is it normal?
  Is it poisson?
  Chi Square?
  
Boxplots of States by Years

#####1.2.1.1.1 Total

######1.2.1.1.1.1 Anytime

```{r}
#Poisson Distribution Reasonable?
mean(traffic$totfat)
var(traffic$totfat)

```

```{r}
#Graphical

h = geom_histogram(aes(y=..count..), 
                 bins = 30, fill = "#99123F", 
                 colour = "black")
t = theme(plot.title = element_text(lineheight = 1, face = "bold")) 

plot.hist1  = ggplot(traffic, aes(x = totfat)) + scale_x_continuous(name ="Total Deaths") + h + t 
plot.hist2  = ggplot(traffic, aes(x = nghtfat)) + scale_x_continuous(name ="Night Deaths") + h + t
plot.hist3  = ggplot(traffic, aes(x = wkndfat)) + scale_x_continuous(name ="Weekend Deaths") + h + t + t
plot.hist4  = ggplot(traffic, aes(x = totfatpvm)) + scale_x_continuous(name ="Total/Mile Driven") + h + t 
plot.hist5  = ggplot(traffic, aes(x = nghtfatpvm)) + scale_x_continuous(name ="Night/Mile Driven") + h + t + t
plot.hist6  = ggplot(traffic, aes(x = wkndfatpvm)) + scale_x_continuous(name ="Weekend/Mile Driven") + h + t + t
plot.hist7  = ggplot(traffic, aes(x = totfatrte)) + scale_x_continuous(name ="Total/Capita") + h + t + t
plot.hist8  = ggplot(traffic, aes(x = nghtfatrte)) + scale_x_continuous(name ="Night/Capita") + h + t + t
plot.hist9  = ggplot(traffic, aes(x = wkndfatrte)) + scale_x_continuous(name ="Weekend/Capita") + h + t 
grid.arrange(plot.hist1, plot.hist2, plot.hist3, plot.hist4, plot.hist5, plot.hist6, plot.hist7, 
             plot.hist8, plot.hist9, nrow = 3, ncol = 3, top = quote("Histograms"))  
```

```{r}
states = map_data('state', projection = "albers", parameters = c(39, 45))
statenames = unique(states$region[])
statenames = c(statenames[1], "alaska", statenames[2:10], 
               'hawaii', statenames[11:length(statenames)])
states = states[states$region != "district of columbia",]
states$state = match(states$region,statenames)

traffic.map = merge(states, traffic, by="state")


b = geom_boxplot(aes(fill = state, group = state)) 
t = theme(plot.title = element_text(lineheight = 1, face = "bold"), 
          legend.position = "none", 
          axis.text.x = element_blank(),
          axis.title.x = element_blank()) 
c = scale_fill_gradientn(limits=c(0,51), breaks=seq(0, 40, by=10), colours=rainbow(5))

plot.bp1 = ggplot(traffic.map, aes(state, totfat)) + b + t + c
plot.bp2 = ggplot(traffic.map, aes(state, nghtfat)) + b + t + c
plot.bp3 = ggplot(traffic.map, aes(state, wkndfat)) + b + t + c
plot.bp4 = ggplot(traffic.map, aes(state, totfatpvm)) + b + t + c
plot.bp5 = ggplot(traffic.map, aes(state, nghtfatpvm)) + b + t + c
plot.bp6 = ggplot(traffic.map, aes(state, wkndfatpvm)) + b + t + c
plot.bp7 = ggplot(traffic.map, aes(state, totfatrte)) + b + t + c
plot.bp8 = ggplot(traffic.map, aes(state, nghtfatrte)) + b + t + c
plot.bp9 = ggplot(traffic.map, aes(state, wkndfatrte)) + b + t + c
grid.arrange(plot.bp1, plot.bp2, plot.bp3, plot.bp4, plot.bp5, plot.bp6, plot.bp7, 
             plot.bp8, plot.bp9, nrow = 3, ncol = 3) 

```

```{r}
b = geom_boxplot(aes(fill = year, group = year)) 
t = theme(plot.title = element_text(lineheight = 1, face = "bold"), 
          legend.position = "none",
          axis.text.x = element_blank(),
          axis.title.x = element_blank()) 
#install.packages("viridis")

c = scale_fill_viridis()
#?scale_color_viridis
plot.bp1 = ggplot(traffic.map, aes(year, totfat)) + b + t + c
plot.bp2 = ggplot(traffic.map, aes(year, nghtfat)) + b + t + c
plot.bp3 = ggplot(traffic.map, aes(year, wkndfat)) + b + t + c
plot.bp4 = ggplot(traffic.map, aes(year, totfatpvm)) + b + t + c
plot.bp5 = ggplot(traffic.map, aes(year, nghtfatpvm)) + b + t + c
plot.bp6 = ggplot(traffic.map, aes(year, wkndfatpvm)) + b + t + c
plot.bp7 = ggplot(traffic.map, aes(year, totfatrte)) + b + t + c
plot.bp8 = ggplot(traffic.map, aes(year, nghtfatrte)) + b + t + c
plot.bp9 = ggplot(traffic.map, aes(year, wkndfatrte)) + b + t + c
grid.arrange(plot.bp1, plot.bp2, plot.bp3, plot.bp4, plot.bp5, plot.bp6, plot.bp7, 
             plot.bp8, plot.bp9, nrow = 3, ncol = 3)
```

```{r}
states = map_data('state', projection = "albers", parameters = c(39, 45))
statenames = unique(states$region[])
statenames = c(statenames[1], "alaska", statenames[2:10], 
               'hawaii', statenames[11:length(statenames)])
states = states[states$region != "district of columbia",]
states$state = match(states$region,statenames)
traffic.map = merge(states, traffic, by="state")

no_var = !names(traffic.map) %in% c('year', "region", "subregion", "lat", "long", "order", "group")
traffic.state.agg = aggregate(traffic.map[, no_var], list(traffic.map$state), mean)
traffic.map.agg = merge(states, traffic.state.agg, by="state")

t = theme(plot.title = element_text(lineheight = 1, face = "bold"), 
          legend.position = "none",
          axis.text.y = element_blank(),
          axis.title.y = element_blank())

plot.map1 = qplot(long, lat, data=traffic.map.agg, geom="polygon", fill=totfat, group=group) + labs(x = "Total") + t
plot.map2 = qplot(long, lat, data=traffic.map.agg, geom="polygon", fill=nghtfat, group=group) + labs(x = "Night") + t
plot.map3 = qplot(long, lat, data=traffic.map.agg, geom="polygon", fill=wkndfat, group=group) + labs(x = "Weekend") + t
plot.map4 = qplot(long, lat, data=traffic.map.agg, geom="polygon", fill=totfatpvm, group=group) + labs(x = "Total/Mile Driven") + t
plot.map5 = qplot(long, lat, data=traffic.map.agg, geom="polygon", fill=nghtfatpvm, group=group) + labs(x = "Night/Mile Driven") + t
plot.map6 = qplot(long, lat, data=traffic.map.agg, geom="polygon", fill=wkndfatpvm, group=group) + labs(x = "Weekend/Mile Driven") + t
plot.map7 = qplot(long, lat, data=traffic.map.agg, geom="polygon", fill=totfatrte, group=group) + labs(x = "Total/Capita") + t
plot.map8 = qplot(long, lat, data=traffic.map.agg, geom="polygon", fill=nghtfatrte, group=group) + labs(x = "Night/Capita") + t
plot.map9 = qplot(long, lat, data=traffic.map.agg, geom="polygon", fill=wkndfatrte, group=group) + labs(x = "Weekend/Capita") + t
grid.arrange(plot.map1, plot.map2, plot.map3, plot.map4, plot.map5, plot.map6, plot.map7, 
             plot.map8, plot.map9, nrow = 3, ncol = 3, top = quote("Average Traffic Fatalaties 1980-2004"))

#Tabular

#describe(traffic$totfat)
#shapiro.test(log(traffic$totfat))
```
###1.2.2 Bivariate EDA

####1.2.2.1 Independent - Independent
Is there any indication of multi-collinearity?

```{r}
sum_by_year = aggregate(traffic[,c('bac08','bac10','sbprim','sbsecon','perse','sl70plus','gdl')], traffic['year'], FUN=sum)
sum_by_state = aggregate(traffic[,c('bac08','bac10','sbprim','sbsecon','perse','sl70plus','gdl')], traffic['state'], FUN=sum)
#install.packages("GGally")
library(GGally)
ggpairs(sum_by_year)
ggpairs(sum_by_state)
```

####1.2.2.2 Dependent - Independent 
#####1.2.2.2.1 First-Order Interactions
###### Fatality Rate vs. Unemployment, Perc14_24, vehicmilespc
None of the bivariate charts show any patterns that *strongly* suggest that we should transform one variable or the other.  


```{r}
ggplot(traffic, aes(x=unem, y=totfatrte)) + geom_point()
ggplot(traffic, aes(x=perc14_24, y=totfatrte)) + geom_point()
ggplot(traffic, aes(x=vehicmilespc, y=totfatrte)) + geom_point()

modfit1 = lm(totfatrte~unem, traffic)
par(mfrow = c(2,2))
plot(modfit1)
summary(modfit1)

modfit2 = lm(totfatrte~perc14_24, traffic)
par(mfrow = c(2,2))
plot(modfit2)
summary(modfit2)

modfit3 = lm(totfatrte~vehicmilespc, traffic)
par(mfrow = c(2,2))
plot(modfit3)
summary(modfit3)

```

#####1.2.2.2.2 Higher-Order Interactions
This is for continuous variables only.

#Question 2
****************
2. How is the our dependent variable of interest totfatrte defined? What is the average of this variable in each of the years in the time period covered in this dataset? Estimate a very simple regression model of totfatrte on dummy variables for the years 1981 through 2004. What does this model explain? Describe what you find in this model. Did driving become safer over this period? Please provide a detailed explanation.
****************

##2.1 Totfatrte Definition
How is the our dependent variable of interest totfatrte defined?
The $totfatrte$ is defined as fatalities per 100K population. 

##2.2 Totfatrte Average
What is the average of this variable in each of the years in the time period covered in this dataset?   
```{r}
avg_by_year = aggregate(totfatrte~year, traffic, mean)
qplot(x=year, y=totfatrte, data=avg_by_year) + geom_text(aes(label=round(totfatrte,1)),hjust=0, vjust=0)
```

Over the years, we see a gradual decrease in the average $totfatrte$ variable - from a high of 25.49 in 1980 to a low of 16.73 in 2004.  We do see two steep declines in the rate of fatalities (in the early 80's and then again between late 80's and early 90's),  since then we have only seen a gradual decrease.  It also appears there are periods when we see an increase in the average fatality rate  (albeit small) mid-80's,  '92-'95 and 2002.  

##2.3 Totfatrte Regression  

###2.3.1 Modeling
Estimate a very simple regression model of totfatrte on dummy variables for the years 1981 through 2004. 
```{r}
lm.mod = lm(totfatrte~d81+d82+d83+d84+d85+d86+d87+d88+d89+
                      d90+d91+d92+d93+d94+d95+d96+d97+d98+d99+
                      d00+d01+d02+d03+d04, data=traffic)
summary(lm.mod)
mean(traffic$totfatrte[traffic$d80 == 1])
```

###2.3.2 Model Explanation
What does this model explain? 

The regression explains how the  fatality rate has changed (average across the 48 states) as compared to the year 1980.  For example, d04 has a coefficient of -8.766.  This implies that the average of $totfatrte$ was 8.8 less than 25.5 (average of $totfatrte$ in 1980) or 16.8, precisely what is reported in the graph above.

###2.3.3 Model Findings
Describe what you find in this model. Did driving become safer over this period? Please provide a detailed explanation.  

The fitted regression equation is $avg_fatality = 25.49 + (-1.824)\cdot d81+ (-4.552) \cdot d82 + \cdots + + (-8.766) \cdot d04 $  
All the estimated coefficients are statistically significant at 5%, except for the one for 1981, which suggests that there was a lot of variability in fatality rates across the 48 states in 1981. 

The intercept of the regression model is the average fatality rate for 1980. All other coefficients measure how the average fatality compares for the year (represented by the dummy variable) versus 1980.  Each coefficient is negative, which means that the average fatality rate each year decreased relative to 1980.  The coefficients are also mostly increasingly negative, represting the negative trend that we see in the graph above.  *Based on this, we would say that driving (as measured by fatality rate per 100K population) has gotten safer over the years 1980-2004 on average in the United States.*  Note, we have not accounted for any other factors in this assessment.  The target variable $totfatrte$ is a simple average of the fatality rates of each state, which is not weighted by the population of each state, nor for the cumulative miles driven.  


#Question 3
****************
3. Expand your model in Exercise 2 by adding variables bac08, bac10, perse, sbprim, sbsecon, sl70plus, gdl, perc14_24, unem, vehicmilespc, and perhaps transformations of some or all of these variables. Please explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. How are the variables bac8 and bac10 defined? Interpret the coefficients on bac8 and bac10. Do per se laws have a negative effect on the fatality rate? What about having a primary seat belt law? (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)****************

##3.1 Model Expansion
Expand your model in Exercise 2 by adding variables bac08, bac10, perse, sbprim, sbsecon, sl70plus, gdl, perc14_24, unem, vehicmilespc, and perhaps transformations of some or all of these variables.

```{r}
traffic$bac08bin = ifelse(traffic$bac08<0.5,0,1)
traffic$bac10bin = ifelse(traffic$bac10<=0.5,0,1)
traffic$persebin = ifelse(traffic$perse<0.5,0,1)
traffic$sbprimbin = ifelse(traffic$sbprim<0.5,0,1)
traffic$sbseconbin = ifelse(traffic$sbsecon<0.5,0,1)
traffic$sl70plusbin = ifelse(traffic$sl70plus<0.5,0,1)
traffic$gdlbin = ifelse(traffic$gdl<0.5,0,1)

traffic$bac0810bin = traffic$bac08bin + traffic$bac10bin
describe(traffic$bac0810bin)

```


```{r}

lm.mod2 = lm(totfatrte~bac08bin+bac10bin+persebin+sbprimbin+sbseconbin+sl70plusbin+gdlbin+
                      perc14_24+unem+vehicmilespc+
                      d81+d82+d83+d84+d85+d86+d87+d88+d89+
                      d90+d91+d92+d93+d94+d95+d96+d97+d98+d99+
                      d00+d01+d02+d03+d04, data=traffic)
summary(lm.mod2)
plot(lm.mod2)
```



##3.2 Rationale
Please explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. 

In most instances we can expect there to be a lag between when certain types of laws are enacted and when they impact  change in behavior.  Given that, we suggest transforming the variables that are essentially binary in nature (whether a law was in effect or not) to be strictly binary (so if it was in effect for less than 1/2 the year, we will code as 0 and 1 otherwise).  

As for the other variables - $perc14_24, unem, vehicmilespc$, they are all already expressed as normalized metrics (where the denominator is some measure of the population, either as per 100 people or per person).  




##3.3 BAC
###3.3.1 Variable Definition
How are the variables bac8 and bac10 defined?   

The BAC (blood alcohol content) is a measure used to determine if someone is driving under the influence of alcohol.  For $bac10$ the threshold is 0.10 and $bac8$ represents a threshold of 0.08.  


###3.3.2 Coefficient Interpretation
Interpret the coefficients on bac8 and bac10. 

The coefficient on bac08 is -2.13 and the coefficient for bac10 is -1.12; The mathematical interpretation of the coefficients in the regression would suggest that all else being equal, a state that has a threshold of 0.08 would have a lower $totfatrte$ than a state that has a threshold of 0.1 because the coeff for bac08 is -2.13 and the coeff for bac10 is -1.12.  This model structure  also suggests that the effect of having neither of the thresholds implies an increase of the fatality rate by 3.25.

##3.4 Per Se Laws
Do per se laws have a negative effect on the fatality rate? 

According to the linear regression model - YES, perse laws have a negative effect on $totfatrte$. All else being equal, the existence of perse laws lowers $totfatrte$ by -0.57 compared to when these laws do not exist.  


##3.5 Primary Seat Belt
What about having a primary seat belt law? (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)

Per the model specification, inclusion of the primary seatbelt variable $sbprimbin$ implies a reduction in fatalities by 0.0425. However, because the coefficient is not significantly different from zero, we interpret it as having no effect on $totfatrte$.  This does not imply that seatbelts do not affect fatality rates, just that compulsory laws may not change drivers' willingness to comply with the law.  




#Question 4
****************
4. Reestimate the model from Exercise 3 using a fixed effects (at the state level) model. How do the
coefficients on bac08, bac10, perse, and sbprim compare with the pooled OLS estimates? Which set of
estimates do you think is more reliable? What assumptions are needed in each of these models? Are
these assumptions reasonable in the current context?
****************

##4.1 Fixed Effect Modeling
Reestimate the model from Exercise 3 using a fixed effects (at the state level) model. 
### 


```{r}
library(plm)


plm.mod1 = plm(totfatrte~bac08bin+bac10bin+persebin+sbprimbin+sbseconbin+sl70plusbin+gdlbin+
                      perc14_24+unem+vehicmilespc + d81+d82+d83+d84+d85+d86+d87+d88+d89+
                      d90+d91+d92+d93+d94+d95+d96+d97+d98+d99+d00+d01+d02+d03+d04, 
               data=traffic, model=c("within"), index=c('state','year'), effect = )
summary(plm.mod1)
```


##4.2 Coefficient Comparison
How do the coefficients on bac08, bac10, perse, and sbprim compare with the pooled OLS estimates?  

The coefficients for bac08, bac10, perse and sbprim are all statistically significant with the panel fixed-effects model.  The estimate for the bac08 coeff is slightly lower, the coeff for bac10 is about the same as before. The perse coeff has changed significanty from -6.1 to -1.1;  We also fnid that the coeff for sbprim is now statistically significant.  

##4.3 Assessment of Reliability
Which set of estimates do you think is more reliable?  
Both models explain about the same amount of variation in the $totfatrte$ variable (comparable adjusted-R-sq values);  however the fixed-effects model is more reliable because it models the variation over time in $totfatrte$ and all the independent variables  *within* each state 

##4.4 Modeling Assumptions
What assumptions are needed in each of these models? Are these assumptions reasonable in the current context?

The fixed effect assumption is that the individual-specific effects are correlated with the independent variables.

The pooled effects assumption (made in a random effects model) is that the individual-specific effects are uncorrelated with the independent variables. This is a very strong assumption and a difficult one to meet.  We are assuming there is no omitted variable bias and no correlation between same-state observations.  

#Question 5
****************
5. Would you prefer to use a random effects model instead of the fixed effects model you build in Exercise
4? Why? Why not?
****************

Random effects model would pool everything together and would rely on the assumption that same-state observations are independent.  While we have quite a few variables we could add to the mix, this isn't a realistic assumption to make.  We need to account for cultural/economic differences between states.  We are able to remove this omitted variable bias using fixed effects modeling with dummy variables of each year to explain the unaccounted for time-variant error dependence.

#Question 6
****************
6. Suppose that vehicmilespc, the number of miles driven per capita, increases by 1,000. Using the FE
estimates, what is the estimated effect on totfatrte? Be sure to interpret the estimate as if explaining
to a layperson.
****************

From the fixed-effects regression, the coefficient for the $vehicmilespc$ variable was 0.000942 fatalaties/100K people per mile-driven/capita.  This implies that for all other things held equal, an increase of 1,000 miles driven per capita would result in an increase of fatalaties per 0.942 fatalaties per 100K people.

#Question 7
****************
7. If there is serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would
be the consequences on the coefficient estimates and their standard errors?
****************

If there existed serial correlation in the model errors, we would be too liberal with our confidence intervals for estimations of the model parameters.  That is, we would commit Type I error, rejecting the null hypothesis too easily, saying that our variables are significant when they actually aren't.  

Conversely, with the presence of heteroskedasticity in our error, we may commit Type II error, failing to reject the null hypothesis when it should be rejected, which means we do not detect the significance of a potentially valuable regressor.
